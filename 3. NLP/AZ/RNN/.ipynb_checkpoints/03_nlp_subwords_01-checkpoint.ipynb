{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqHiDh2mxsfu"
   },
   "source": [
    "What's in a (sub)word?\n",
    "In this colab, we'll work with subwords, or words made up of the pieces of larger words, and see how that impacts our network and related embeddings.\n",
    "We’ve worked with full words before for our sentiment models, and found some issues right at the start of the lesson when using character-based tokenization. Subwords are another approach, where individual words are broken up into the more commonly appearing pieces of themselves. This helps avoid marking very rare words as OOV when you use only the most common words in a corpus.\n",
    "\n",
    "As shown in the video, this can further expose an issue affecting all of our models up to this point, in that they don’t understand the full context of the sequence of words in an input. The next lesson on recurrent neural networks will help address this issue.\n",
    "\n",
    "https://video.udacity-data.com/topher/2020/March/5e6fb669_subwords/subwords.png\n",
    "\n",
    "Subword Datasets\n",
    "\n",
    "There are a number of already created subwords datasets available online. If you check out the IMDB dataset on TFDS https://www.tensorflow.org/datasets/catalog/imdb_reviews, for instance, by scrolling down you can see datasets with both 8,000 subwords as well as 32,000 subwords in a corpus (along with regular full-word datasets).\n",
    "\n",
    "But how to creat TensorFlow’s SubwordTextEncoder and its build_from_corpus function to create one from the reviews dataset we used previously is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8Wa_ZlX-mPH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling1D, Flatten\n",
    "from tensorflow.keras.layers import LSTM, Embedding\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJAxrOLi-02C",
    "outputId": "8a3c6f3e-9cef-4318-ad66-bc5827aaa1f6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P \\\n",
    "    -O /tmp/sentiment.csv\n",
    "    \n",
    "    # https://www.kaggle.com/marklvl/sentiment-labelled-sentences-data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FC2Leqipxsf5"
   },
   "outputs": [],
   "source": [
    "path = tf.keras.utils.get_file('sentiment.csv', \n",
    "                               'https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qzVPXLwNRdF",
    "outputId": "d5ce05ee-ec89-4903-9583-2610cdcbfa01"
   },
   "outputs": [],
   "source": [
    "print (path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dr-EDUKP_HBl"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "l11LJeKONXM7",
    "outputId": "8507b1cf-f101-422d-f782-0c68ededec6a"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zut9Wng_R3B"
   },
   "source": [
    "@todo : We can use the existing Amazon and Yelp reviews dataset with `tensorflow_datasets`'s `SubwordTextEncoder` functionality. `SubwordTextEncoder.build_from_corpus()` will create a tokenizer we can use this functionality to get subwords from a much larger corpus of text as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr1FKFBWK2b_"
   },
   "source": [
    "The Amazon and Yelp dataset we are using isn't super large, so we'll create a subword `vocab_size` of only the 1,000 most common words, as well as cutting off each subword to be at most 5 characters. Documentation [here](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder#build_from_corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcEJIxWwK2b_"
   },
   "outputs": [],
   "source": [
    "# note this is the code in the past examples without using subwords\n",
    "#tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "#tokenizer.fit_on_texts(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVuGTmm3K2cA"
   },
   "outputs": [],
   "source": [
    "sentences = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUzd-5YbMggG"
   },
   "outputs": [],
   "source": [
    "labels = df['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFV_wbG_xsgC"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aElsgxia_43g"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 1000\n",
    "MAX_SUBWORD_LENGTH = 5\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(sentences, \n",
    "                                                                      MAX_VOCAB_SIZE, \n",
    "                                                                      MAX_SUBWORD_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JzH9ptyyNne",
    "outputId": "d7aecc3e-50ff-444b-fe34-976e26a71aea"
   },
   "outputs": [],
   "source": [
    "# Check that the tokenizer works appropriately\n",
    "num = 5\n",
    "print(sentences[num])\n",
    "encoded = tokenizer.encode(sentences[num])\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XNZWGKqBDc3",
    "outputId": "e3ac90a5-3282-42b0-9abe-d5fe74805d3a"
   },
   "outputs": [],
   "source": [
    "# Separately print out each subword, decoded\n",
    "for i in encoded:\n",
    "  print(tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAmql34aGfeV"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Replace sentence data with encoded subwords Now, we'll re-create the dataset to be used for training by actually \n",
    "encoding each of the individual sentences. \n",
    "This is equivalent to text_to_sequences with the Tokenizer we used in earlier exercises.\n",
    "'''\n",
    "for i, sentence in enumerate(sentences):\n",
    "  sentences[i] = tokenizer.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNnee_csG5Iz",
    "outputId": "91e9dd4b-8a9d-41f4-ba68-3eb9d3f03a1b"
   },
   "outputs": [],
   "source": [
    "# Check the sentences are appropriately replaced\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INIFSAcEHool"
   },
   "outputs": [],
   "source": [
    "# Before training, we still need to pad the sequences, as well as split into training and test sets.\n",
    "\n",
    "max_length = 50\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "\n",
    "# Pad all sentences\n",
    "sentences_padded = pad_sequences(sentences, maxlen=max_length, \n",
    "                                 padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Separate out the sentences and labels into training and test sets\n",
    "training_size = int(len(sentences) * 0.8)\n",
    "\n",
    "training_sentences = sentences_padded[0:training_size]\n",
    "testing_sentences = sentences_padded[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]\n",
    "\n",
    "# Make labels into numpy arrays for use with the network later\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxuDONtjMnVl",
    "outputId": "1f683b30-21ad-4462-9869-a70f6c081c23"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "# We get to choose embedding dimensionality\n",
    "D = 16 # 20\n",
    "#V = len(word_index)\n",
    "# Hidden state dimensionality\n",
    "# M = 15\n",
    "\n",
    "i = Input(shape=(max_length,))                        # T = 121\n",
    "x = Embedding(MAX_VOCAB_SIZE, embedding_dim)(i)                   # V = 7246   D = 20\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dense(6, activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(i, x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDKcL64IPcfy",
    "outputId": "6b5882a1-b38e-4b30-d915-3156411de8f3"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(MAX_VOCAB_SIZE, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqkMNtIeP3oz",
    "outputId": "1a12860e-a8f6-49da-b892-755860eb84fb"
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(training_sentences, \n",
    "                    training_labels_final, \n",
    "                    epochs=num_epochs, \n",
    "                    validation_data=(testing_sentences, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "uy8KIMPIQlvH",
    "outputId": "638764a1-64a0-400c-d8e5-9027f1d0e909"
   },
   "outputs": [],
   "source": [
    "# Does there appear to be a difference in how validation accuracy and loss is trending compared to with full words?\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dezs4wE5RMQu"
   },
   "outputs": [],
   "source": [
    "# First get the weights of the embedding layer\n",
    "#e = model.layers[0]\n",
    "#weights = e.get_weights()[0]\n",
    "#print(weights.shape) # shape: (MAX_VOCAB_SIZE, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCwKC5LdPPAo",
    "outputId": "c3b19482-7f37-409a-c346-49184ff90c1b"
   },
   "outputs": [],
   "source": [
    "# First get the weights of the embedding layer\n",
    "e = model.layers[1]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (MAX_VOCAB_SIZE, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn1GY2F8xsgu"
   },
   "source": [
    "Note that the below code does have a few small changes to handle the different way text is encoded in our dataset compared to before with the built in `Tokenizer`. You may get an error like \"Number of tensors (999) do not match the number of lines in metadata (992).\" As long as you load the vectors first without error and wait a few seconds after this pops up, you will be able to click outside the file load menu and still view the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXKqy9Z1RSmt"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Write out the embedding vectors and metadata\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(0, MAX_VOCAB_SIZE - 1):\n",
    "  word = tokenizer.decode([word_num])\n",
    "  embeddings = weights[word_num]\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "v04wBMybRoGx",
    "outputId": "b35375d5-b249-4bac-d91d-84e669d4fb28"
   },
   "outputs": [],
   "source": [
    "# Download the files\n",
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "else:\n",
    "  files.download('vecs.tsv')\n",
    "  files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Sentiment in New Reviews Use the model to predict a review   \n",
    "fake_reviews = ['I love this phone', 'I hate spaghetti', \n",
    "                'Everything was cold',\n",
    "                'Everything was hot exactly as I wanted', \n",
    "                'Everything was green', \n",
    "                'the host seated us immediately',\n",
    "                'they gave us free chocolate cake', \n",
    "                'not sure about the wilted flowers on the table',\n",
    "                'only works when I stand on tippy toes',\n",
    "                'bats are all wearing underwears with tie',\n",
    "                'bat was runnig the show',\n",
    "                'does not work when I stand on my head']\n",
    "\n",
    "print(fake_reviews) \n",
    "\n",
    "# Create the sequences\n",
    "padding_type='post'\n",
    "sample_sequences = tokenizer.texts_to_sequences(fake_reviews)\n",
    "fakes_padded = pad_sequences(sample_sequences, padding=padding_type, maxlen=max_length)           \n",
    "\n",
    "print('\\nHOT OFF THE PRESS! HERE ARE SOME NEWLY MINTED, ABSOLUTELY GENUINE REVIEWS!\\n')              \n",
    "\n",
    "classes = model.predict(fakes_padded)\n",
    "\n",
    "# The closer the class is to 1, the more positive the review is deemed to be\n",
    "for x in range(len(fake_reviews)):\n",
    "  print(fake_reviews[x])\n",
    "  print(classes[x])\n",
    "  print('\\n')\n",
    "\n",
    "# Try adding reviews of your own\n",
    "# Add some negative words (such as \"not\") to the good reviews and see what happens\n",
    "# For example:\n",
    "# they gave us free chocolate cake and did not charge us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZV30fXmK2cA"
   },
   "source": [
    "    You’ve already learned an amazing amount of material on Natural Language Processing with TensorFlow in this\n",
    "    lesson.You started with Tokenization by:\n",
    "    \n",
    "    You’ve already learned an amazing amount of material on Natural Language\n",
    "    Processing with TensorFlow in this lesson.\n",
    "    You started with Tokenization by:\n",
    "    Tokenizing input text\n",
    "    Creating and padding sequences\n",
    "    Incorporating out of vocabulary words\n",
    "    Generalizing tokenization and sequence methods to real world datasets\n",
    "    \n",
    "    From there, you moved onto Embeddings, where you:\n",
    "\n",
    "    transformed tokenized sequences into embeddings\n",
    "    developed a basic sentiment analysis model\n",
    "    visualized the embeddings vector\n",
    "    tweaked hyperparameters of the model to improve it\n",
    "    and diagnosed potential issues with using pre-trained subword tokenizers when the network doesn’t have sequence context \n",
    "    In the next lesson, you’ll dive into Recurrent Neural Networks, which will be able to understand the sequence of\n",
    "    inputs, and you'll learn how to generate new text."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "05_nlp_subwords-01.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
