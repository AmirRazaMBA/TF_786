{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2LmLTREBf5ng"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lyrics_corpus(dataset, field):\n",
    "  # Remove all other punctuation\n",
    "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
    "  # Make it lowercase\n",
    "  dataset[field] = dataset[field].str.lower()\n",
    "  # Make it one long string to split by line\n",
    "  lyrics = dataset[field].str.cat()\n",
    "  corpus = lyrics.split('\\n')\n",
    "  # Remove any trailing whitespace\n",
    "  for l in range(len(corpus)):\n",
    "    corpus[l] = corpus[l].rstrip()\n",
    "  # Remove any empty lines\n",
    "  corpus = [l for l in corpus if l != '']\n",
    "\n",
    "  return corpus\n",
    "\n",
    "def tokenize_corpus(corpus, num_words=-1):\n",
    "  # Fit a Tokenizer on the corpus\n",
    "  if num_words > -1:\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "  else:\n",
    "    tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apcEXp7WhVBs",
    "outputId": "987eca43-d84a-4f06-f7bd-e006ad0b87c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\.keras\\datasets\\songdata.csv\n",
      "513\n"
     ]
    }
   ],
   "source": [
    "# Step 1 : Core of the process tokenize the words\n",
    "# Read the dataset from csv - just first 10 songs for now\n",
    "    \n",
    "path = tf.keras.utils.get_file('songdata.csv', \n",
    "                               'https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8')\n",
    "print (path)\n",
    "      \n",
    "dataset = pd.read_csv(path, dtype=str)[:10]\n",
    "\n",
    "# Create the corpus using the 'text' column containing lyrics\n",
    "corpus = create_lyrics_corpus(dataset, 'text')\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenizer = tokenize_corpus(corpus)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 19  # why 19?\n",
    "\n",
    "#print(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>look at her face its a wonderful face  \\nand i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>take it easy with me please  \\ntouch me gently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>ill never know why i had to go  \\nwhy i had to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  look at her face its a wonderful face  \\nand i...  \n",
       "1  take it easy with me please  \\ntouch me gently...  \n",
       "2  ill never know why i had to go  \\nwhy i had to...  \n",
       "3  making somebody happy is a question of give an...  \n",
       "4  making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QmlTsUqfikVO"
   },
   "outputs": [],
   "source": [
    "# Step 2: The word sequence and label on which to train\n",
    "sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tsequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences for equal input length \n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
    "input_sequences  = sequences[:,:-1] \n",
    "labels = sequences[:,-1]\n",
    "# One-hot encode the labels\n",
    "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "x = input_sequences\n",
    "y = one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zsmu3aEId49i",
    "outputId": "87f5d91d-bab6-4c53-ad7f-73c2fed5c5b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "97\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
      "   4]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
      " 287]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Check out how some of our data is being stored\n",
    "# The Tokenizer has just a single index per word\n",
    "print(tokenizer.word_index['know'])\n",
    "print(tokenizer.word_index['feeling'])\n",
    "# Input sequences will have multiple indexes\n",
    "print(x[5])\n",
    "print(x[6])\n",
    "# And the one hot labels will be as long as the full spread of tokenized words\n",
    "print(y[5])\n",
    "print(y[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "G1YXuxIqfygN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 19, 64)            32832     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 40)                13600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 513)               21033     \n",
      "=================================================================\n",
      "Total params: 67,465\n",
      "Trainable params: 67,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 6.0515 - accuracy: 0.0323ETA: 0s - los\n",
      "Epoch 2/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.4524 - accuracy: 0.0378\n",
      "Epoch 3/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.3759 - accuracy: 0.0383\n",
      "Epoch 4/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 5.3214 - accuracy: 0.0399\n",
      "Epoch 5/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.2480 - accuracy: 0.0363\n",
      "Epoch 6/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.1848 - accuracy: 0.0399\n",
      "Epoch 7/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.1331 - accuracy: 0.0429\n",
      "Epoch 8/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 5.0839 - accuracy: 0.0414\n",
      "Epoch 9/200\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 5.0267 - accuracy: 0.0525\n",
      "Epoch 10/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.9719 - accuracy: 0.0560\n",
      "Epoch 11/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.9044 - accuracy: 0.0489\n",
      "Epoch 12/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.8279 - accuracy: 0.0565\n",
      "Epoch 13/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.7426 - accuracy: 0.0560\n",
      "Epoch 14/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.6540 - accuracy: 0.0651\n",
      "Epoch 15/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.5698 - accuracy: 0.0787\n",
      "Epoch 16/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.4794 - accuracy: 0.0792\n",
      "Epoch 17/200\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 4.4039 - accuracy: 0.1014\n",
      "Epoch 18/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.3367 - accuracy: 0.1070\n",
      "Epoch 19/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.2657 - accuracy: 0.1160\n",
      "Epoch 20/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.1993 - accuracy: 0.1307\n",
      "Epoch 21/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.1318 - accuracy: 0.1342\n",
      "Epoch 22/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.0965 - accuracy: 0.1352\n",
      "Epoch 23/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.0115 - accuracy: 0.1539\n",
      "Epoch 24/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.9558 - accuracy: 0.1630\n",
      "Epoch 25/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.8942 - accuracy: 0.1786\n",
      "Epoch 26/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 3.8326 - accuracy: 0.1821\n",
      "Epoch 27/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 3.7768 - accuracy: 0.1973\n",
      "Epoch 28/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 3.7210 - accuracy: 0.1988\n",
      "Epoch 29/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 3.6665 - accuracy: 0.2286\n",
      "Epoch 30/200\n",
      "62/62 [==============================] - ETA: 0s - loss: 3.6253 - accuracy: 0.21 - 1s 12ms/step - loss: 3.6206 - accuracy: 0.2240\n",
      "Epoch 31/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.5775 - accuracy: 0.2351\n",
      "Epoch 32/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.5189 - accuracy: 0.2508\n",
      "Epoch 33/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.4708 - accuracy: 0.2533\n",
      "Epoch 34/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 3.4224 - accuracy: 0.2714\n",
      "Epoch 35/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.3646 - accuracy: 0.2856\n",
      "Epoch 36/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.3168 - accuracy: 0.2957\n",
      "Epoch 37/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.2749 - accuracy: 0.3063\n",
      "Epoch 38/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.2297 - accuracy: 0.3169\n",
      "Epoch 39/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.1802 - accuracy: 0.3295\n",
      "Epoch 40/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.1365 - accuracy: 0.3340\n",
      "Epoch 41/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.0903 - accuracy: 0.3385\n",
      "Epoch 42/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.0622 - accuracy: 0.3441\n",
      "Epoch 43/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 3.0311 - accuracy: 0.3507\n",
      "Epoch 44/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 2.9969 - accuracy: 0.3532\n",
      "Epoch 45/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 2.9351 - accuracy: 0.3602\n",
      "Epoch 46/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.8811 - accuracy: 0.3799\n",
      "Epoch 47/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 2.8310 - accuracy: 0.3835\n",
      "Epoch 48/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 2.7899 - accuracy: 0.3905\n",
      "Epoch 49/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7440 - accuracy: 0.4001\n",
      "Epoch 50/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 2.7059 - accuracy: 0.4036\n",
      "Epoch 51/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.6618 - accuracy: 0.4157\n",
      "Epoch 52/200\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 2.6151 - accuracy: 0.4359\n",
      "Epoch 53/200\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 2.5788 - accuracy: 0.4379\n",
      "Epoch 54/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 2.5744 - accuracy: 0.4445\n",
      "Epoch 55/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.5136 - accuracy: 0.4571\n",
      "Epoch 56/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4668 - accuracy: 0.4591\n",
      "Epoch 57/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.4233 - accuracy: 0.4733\n",
      "Epoch 58/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.3829 - accuracy: 0.4788\n",
      "Epoch 59/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.3624 - accuracy: 0.4818\n",
      "Epoch 60/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3252 - accuracy: 0.4929\n",
      "Epoch 61/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2960 - accuracy: 0.4945\n",
      "Epoch 62/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 2.2494 - accuracy: 0.5061\n",
      "Epoch 63/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2121 - accuracy: 0.5136\n",
      "Epoch 64/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1798 - accuracy: 0.5272\n",
      "Epoch 65/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 2.1398 - accuracy: 0.5328\n",
      "Epoch 66/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 2.1169 - accuracy: 0.5313\n",
      "Epoch 67/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0776 - accuracy: 0.5409\n",
      "Epoch 68/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 2.0387 - accuracy: 0.5545\n",
      "Epoch 69/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0028 - accuracy: 0.5661\n",
      "Epoch 70/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9748 - accuracy: 0.5757\n",
      "Epoch 71/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9417 - accuracy: 0.5807\n",
      "Epoch 72/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.9176 - accuracy: 0.5898\n",
      "Epoch 73/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8838 - accuracy: 0.6004\n",
      "Epoch 74/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8730 - accuracy: 0.6054\n",
      "Epoch 75/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8491 - accuracy: 0.6085\n",
      "Epoch 76/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8431 - accuracy: 0.5999\n",
      "Epoch 77/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.8345 - accuracy: 0.5974\n",
      "Epoch 78/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7979 - accuracy: 0.6095\n",
      "Epoch 79/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7658 - accuracy: 0.6251\n",
      "Epoch 80/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7404 - accuracy: 0.6216 0s - loss:\n",
      "Epoch 81/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7067 - accuracy: 0.6362\n",
      "Epoch 82/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7056 - accuracy: 0.6266\n",
      "Epoch 83/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 1.6589 - accuracy: 0.6448\n",
      "Epoch 84/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6220 - accuracy: 0.6594\n",
      "Epoch 85/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.6032 - accuracy: 0.6539\n",
      "Epoch 86/200\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 1.5716 - accuracy: 0.6670\n",
      "Epoch 87/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.5442 - accuracy: 0.6726\n",
      "Epoch 88/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5220 - accuracy: 0.6791\n",
      "Epoch 89/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 1.4982 - accuracy: 0.6821\n",
      "Epoch 90/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 1.4749 - accuracy: 0.6902\n",
      "Epoch 91/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4601 - accuracy: 0.6927\n",
      "Epoch 92/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4306 - accuracy: 0.6983\n",
      "Epoch 93/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4106 - accuracy: 0.7023\n",
      "Epoch 94/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3905 - accuracy: 0.7028\n",
      "Epoch 95/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.3746 - accuracy: 0.7089\n",
      "Epoch 96/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.3639 - accuracy: 0.7023\n",
      "Epoch 97/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.3500 - accuracy: 0.7164\n",
      "Epoch 98/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.3418 - accuracy: 0.7094\n",
      "Epoch 99/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.3124 - accuracy: 0.7200\n",
      "Epoch 100/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3086 - accuracy: 0.7164 0s - loss: 1\n",
      "Epoch 101/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 1.2953 - accuracy: 0.7185\n",
      "Epoch 102/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2718 - accuracy: 0.7230\n",
      "Epoch 103/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 1.2534 - accuracy: 0.7240\n",
      "Epoch 104/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2403 - accuracy: 0.7265\n",
      "Epoch 105/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 1.2278 - accuracy: 0.7296\n",
      "Epoch 106/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2067 - accuracy: 0.7326\n",
      "Epoch 107/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1994 - accuracy: 0.7326\n",
      "Epoch 108/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 1.1748 - accuracy: 0.7427\n",
      "Epoch 109/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1667 - accuracy: 0.7386\n",
      "Epoch 110/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1475 - accuracy: 0.7472\n",
      "Epoch 111/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.1295 - accuracy: 0.7472\n",
      "Epoch 112/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.1182 - accuracy: 0.7503\n",
      "Epoch 113/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.1005 - accuracy: 0.7563\n",
      "Epoch 114/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.0996 - accuracy: 0.7563\n",
      "Epoch 115/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.0842 - accuracy: 0.7674\n",
      "Epoch 116/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.0808 - accuracy: 0.7649\n",
      "Epoch 117/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.0695 - accuracy: 0.7619\n",
      "Epoch 118/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.0646 - accuracy: 0.7649\n",
      "Epoch 119/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.0407 - accuracy: 0.7709\n",
      "Epoch 120/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.0291 - accuracy: 0.7674\n",
      "Epoch 121/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.0132 - accuracy: 0.7709\n",
      "Epoch 122/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.0031 - accuracy: 0.7750\n",
      "Epoch 123/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9961 - accuracy: 0.7755\n",
      "Epoch 124/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.9871 - accuracy: 0.7775\n",
      "Epoch 125/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.9825 - accuracy: 0.7719\n",
      "Epoch 126/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9807 - accuracy: 0.7704\n",
      "Epoch 127/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.9569 - accuracy: 0.7790\n",
      "Epoch 128/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 0.9430 - accuracy: 0.7825\n",
      "Epoch 129/200\n",
      "62/62 [==============================] - 1s 21ms/step - loss: 0.9321 - accuracy: 0.7896\n",
      "Epoch 130/200\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.9236 - accuracy: 0.7891\n",
      "Epoch 131/200\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 0.9103 - accuracy: 0.7936\n",
      "Epoch 132/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.9019 - accuracy: 0.7936\n",
      "Epoch 133/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8933 - accuracy: 0.7962\n",
      "Epoch 134/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.8897 - accuracy: 0.7952\n",
      "Epoch 135/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8920 - accuracy: 0.7972 0s - loss: 0.8840 - accuracy: 0.80\n",
      "Epoch 136/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.9011 - accuracy: 0.7866\n",
      "Epoch 137/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9060 - accuracy: 0.7906\n",
      "Epoch 138/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8740 - accuracy: 0.7947\n",
      "Epoch 139/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8499 - accuracy: 0.8032\n",
      "Epoch 140/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.8374 - accuracy: 0.8047\n",
      "Epoch 141/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8276 - accuracy: 0.8047\n",
      "Epoch 142/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8190 - accuracy: 0.8052 0s - loss: 0.7\n",
      "Epoch 143/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.8094 - accuracy: 0.8063\n",
      "Epoch 144/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8038 - accuracy: 0.8068\n",
      "Epoch 145/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8005 - accuracy: 0.8108\n",
      "Epoch 146/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.8001 - accuracy: 0.8078\n",
      "Epoch 147/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.7959 - accuracy: 0.8153\n",
      "Epoch 148/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7797 - accuracy: 0.8073\n",
      "Epoch 149/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7794 - accuracy: 0.8123\n",
      "Epoch 150/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7761 - accuracy: 0.8148\n",
      "Epoch 151/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.7590 - accuracy: 0.8163\n",
      "Epoch 152/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7516 - accuracy: 0.8199\n",
      "Epoch 153/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.7555 - accuracy: 0.8169\n",
      "Epoch 154/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.7659 - accuracy: 0.8138\n",
      "Epoch 155/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.7400 - accuracy: 0.8239\n",
      "Epoch 156/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.7385 - accuracy: 0.8234\n",
      "Epoch 157/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.7362 - accuracy: 0.8244\n",
      "Epoch 158/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.7870 - accuracy: 0.8143\n",
      "Epoch 159/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.7478 - accuracy: 0.8169\n",
      "Epoch 160/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.7136 - accuracy: 0.8300\n",
      "Epoch 161/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.6985 - accuracy: 0.8300\n",
      "Epoch 162/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.6888 - accuracy: 0.8375\n",
      "Epoch 163/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.6766 - accuracy: 0.8391\n",
      "Epoch 164/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.6719 - accuracy: 0.8385\n",
      "Epoch 165/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.6693 - accuracy: 0.8385\n",
      "Epoch 166/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.6644 - accuracy: 0.8370\n",
      "Epoch 167/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.6529 - accuracy: 0.8476\n",
      "Epoch 168/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.6449 - accuracy: 0.8451\n",
      "Epoch 169/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.6393 - accuracy: 0.8446\n",
      "Epoch 170/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.6337 - accuracy: 0.8507\n",
      "Epoch 171/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6447 - accuracy: 0.8446\n",
      "Epoch 172/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6526 - accuracy: 0.8451\n",
      "Epoch 173/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 0.6316 - accuracy: 0.8542\n",
      "Epoch 174/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.6212 - accuracy: 0.8496\n",
      "Epoch 175/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6091 - accuracy: 0.8572 0s - loss: 0.5988 - accura\n",
      "Epoch 176/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6032 - accuracy: 0.8602\n",
      "Epoch 177/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.5976 - accuracy: 0.8577\n",
      "Epoch 178/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5955 - accuracy: 0.8607\n",
      "Epoch 179/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5898 - accuracy: 0.8557\n",
      "Epoch 180/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5865 - accuracy: 0.8582\n",
      "Epoch 181/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.6027 - accuracy: 0.8532\n",
      "Epoch 182/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.6009 - accuracy: 0.8542\n",
      "Epoch 183/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.5808 - accuracy: 0.8607\n",
      "Epoch 184/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.5728 - accuracy: 0.8678\n",
      "Epoch 185/200\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 0.5741 - accuracy: 0.8562\n",
      "Epoch 186/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.5615 - accuracy: 0.8668\n",
      "Epoch 187/200\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 0.5540 - accuracy: 0.8658\n",
      "Epoch 188/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.5491 - accuracy: 0.8678\n",
      "Epoch 189/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.5445 - accuracy: 0.8698\n",
      "Epoch 190/200\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 0.5397 - accuracy: 0.8708\n",
      "Epoch 191/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5343 - accuracy: 0.8729\n",
      "Epoch 192/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.5396 - accuracy: 0.8729\n",
      "Epoch 193/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5323 - accuracy: 0.8678\n",
      "Epoch 194/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5423 - accuracy: 0.8643\n",
      "Epoch 195/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5380 - accuracy: 0.8688\n",
      "Epoch 196/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5303 - accuracy: 0.8658\n",
      "Epoch 197/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5246 - accuracy: 0.8713\n",
      "Epoch 198/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.5338 - accuracy: 0.8729\n",
      "Epoch 199/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5384 - accuracy: 0.8683\n",
      "Epoch 200/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6375 - accuracy: 0.8300\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(x, y, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "aeSNfS7uhch0",
    "outputId": "1b507817-69dc-401d-fa71-549bac6626c3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DC7zfcgviDTp",
    "outputId": "22b66389-e2c7-4e1c-8759-f91963d75364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im feeling chills me sing and you make me strong cause i do what could i do could be had break had break had had had had had had had had must joe park park power kind me power truth future do never cassandra i think youll wanted power power power final park park joe future do never smile butterflies shining above you came out that gonna take i think youll cry on out need out hardly neighbours secrets smart hearts power park park feeling feeling neighbours would park power park park down together feet feet park kind do kind out hour kisses\n"
     ]
    }
   ],
   "source": [
    "# Step 3 : Predict new words\n",
    "seed_text = \"im feeling chills\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
