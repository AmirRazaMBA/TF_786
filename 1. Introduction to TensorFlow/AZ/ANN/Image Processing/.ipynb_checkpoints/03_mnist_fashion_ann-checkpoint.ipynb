{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%204%20-%20Lesson%202%20-%20Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3KzJyjv3rnA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmxkHFpt31bM"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTdRgExe4TRB"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train_original = x_train\n",
    "x_test_original = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPc9d3gJ3jWF"
   },
   "outputs": [],
   "source": [
    "y =  ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "np.set_printoptions(linewidth=200)\n",
    "\n",
    "print(x_train[0])\n",
    "plt.imshow(x_train[0])\n",
    "print(y_train[0])\n",
    "print(y[y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRH19pWs6ZDn"
   },
   "outputs": [],
   "source": [
    "x_train  = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFBq7uDnKs19"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Flatten\n",
    "\n",
    "# Build the model\n",
    "\n",
    "i = Input(shape=x_train[0].shape)\n",
    "x = Flatten()(i)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(i, x)\n",
    "model.summary()\n",
    "\n",
    "# Note if a model is built without the input then you can't print the model.summary\n",
    "#model = Sequential()   \n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(128, activation = 'relu'))\n",
    "#model.add(Dense(10, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLMdl9aP8nQ0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LGV2Kloh1Ju"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzlqsEzX9s5P"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBqRwauvfuOi"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "p_test =  np.argmax(model.predict(x_test), axis=-1) \n",
    "print(classification_report(y_test,p_test))\n",
    "print(confusion_matrix(y_test,p_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UhyYEsLgouG"
   },
   "outputs": [],
   "source": [
    "# Show some misclassified examples\n",
    "misclassified_idx = np.where(p_test != y_test)[0]\n",
    "i = np.random.choice(misclassified_idx)\n",
    "plt.imshow(x_test[i], cmap='gray')\n",
    "#plt.title(\"True label: %s Predicted: %s\" % (y_test[i], p_test[i]));\n",
    "plt.title(\"True label: %s  Predicted: %s\" % (y[y_test[i]], y[p_test[i]]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSZSwV5UObQP"
   },
   "outputs": [],
   "source": [
    "# Exercise 2: \n",
    "# Experiment with different values for the dense layer with 512 neurons. \n",
    "# What different results do you get for loss, training time etc? Why do you think that's the case? \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "p_test =  np.argmax(model.predict(x_test), axis=-1) \n",
    "print(classification_report(y_test,p_test))\n",
    "print(confusion_matrix(y_test,p_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Increase to 1024 Neurons -- What's the impact?\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "p_test =  np.argmax(model.predict(x_test), axis=-1) \n",
    "print(classification_report(y_test,p_test))\n",
    "print(confusion_matrix(y_test,p_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExNxCwhcQ18S"
   },
   "outputs": [],
   "source": [
    "# Exercise 3: \n",
    "# What would happen if you remove the Flatten() layer. Why do you think that's the case? \n",
    "\n",
    "model = tf.keras.models.Sequential([#tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "p_test =  np.argmax(model.predict(x_test), axis=-1) \n",
    "print(classification_report(y_test,p_test))\n",
    "print(confusion_matrix(y_test,p_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqoCR-ieSGDg"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMckVntcSPvo"
   },
   "outputs": [],
   "source": [
    "# Exercise 4: \n",
    "# What would happen if you had a different amount than 10? \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5, activation ='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "p_test =  np.argmax(model.predict(x_test), axis=-1) \n",
    "print(classification_report(y_test,p_test))\n",
    "print(confusion_matrix(y_test,p_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1YPa6UhS8Es"
   },
   "outputs": [],
   "source": [
    "# Exercise 5: \n",
    "# What will happen if you add another layer between the one with 512 and the final layer with 10. \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "p_test =  np.argmax(model.predict(x_test), axis=-1) \n",
    "print(classification_report(y_test,p_test))\n",
    "print(confusion_matrix(y_test,p_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uE3esj9BURQe"
   },
   "outputs": [],
   "source": [
    "# Exercise 6: \n",
    "# Try 15 epochs -- you'll probably get a model with a much better loss than the one with 5\n",
    "# Try 30 epochs\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=30)\n",
    "\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "p_test =  np.argmax(model.predict(x_test), axis=-1) \n",
    "print(classification_report(y_test,p_test))\n",
    "print(confusion_matrix(y_test,p_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDqNAqrpCNg0"
   },
   "outputs": [],
   "source": [
    "# Exercise 7: \n",
    "# with out normalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.fit(x_train_original, y_train, epochs=5)\n",
    "model.evaluate(x_test_original, y_test, verbose=0)\n",
    "\n",
    "p_test =  np.argmax(model.predict(x_test_original), axis=-1) \n",
    "print(classification_report(y_test,p_test))\n",
    "print(confusion_matrix(y_test,p_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkaEHHgqZbYv"
   },
   "outputs": [],
   "source": [
    "# Exercise 8: \n",
    "# call back\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('loss')<0.4):\n",
    "      print(\"\\nReached 60% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train=x_train/255.0\n",
    "x_test=x_test/255.0\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=tf.nn.relu))\n",
    "model.add(Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.fit(x_train, y_train, epochs=5, callbacks=[callbacks])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03_image_recognition_without_conv.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
