{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BOwsuGQQY9OL"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Get the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\.keras\\datasets\\irish-lyrics-eof.txt\n"
     ]
    }
   ],
   "source": [
    "path = tf.keras.utils.get_file('irish-lyrics-eof.txt', \n",
    "                               'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt')\n",
    "print (path)\n",
    "\n",
    "data = open(path).read()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Tokenize the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRnDnCW-Z7qv",
    "outputId": "800a9aaa-598e-4b52-8574-fd52330ac14d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2690\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "#print(tokenizer.word_index)\n",
    "print(total_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Create  n-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "soPGVheskaQP"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for line in corpus:\n",
    "    #print(line)\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    #print(token_list)\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        #print(n_gram_sequence)\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Pad sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : X and y Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label one hot encoded\n",
    "x_train  = input_sequences[:,:-1]\n",
    "y_label = input_sequences[:,-1]\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJtwVB2NbOAP",
    "outputId": "29b6b1bd-23ce-4af3-de18-248aa5ff605c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "1\n",
      "71\n",
      "6\n",
      "713\n",
      "39\n",
      "1790\n",
      "1791\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index['in'])\n",
    "print(tokenizer.word_index['the'])\n",
    "print(tokenizer.word_index['town'])\n",
    "print(tokenizer.word_index['of'])\n",
    "print(tokenizer.word_index['athy'])\n",
    "print(tokenizer.word_index['one'])\n",
    "print(tokenizer.word_index['jeremy'])\n",
    "print(tokenizer.word_index['lanigan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49Cv68JOakwv",
    "outputId": "65190eab-183c-4d6f-de40-eb798a34674f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iY-jwvfgbEF8",
    "outputId": "c5d8aad9-9831-4b29-cab8-9911a9fccfbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtzlUMYadhKt",
    "outputId": "a1e71247-6f4d-43f8-eb9e-44cb0b18bb3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0   51   12   96 1217   48\n",
      "    2]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[5])\n",
    "print(y_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4myRpB1c4Gg",
    "outputId": "8f4e39af-8648-4a2b-da86-6c6d86b6828f"
   },
   "outputs": [],
   "source": [
    "#print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9vH8Y59ajYL",
    "outputId": "31315620-92d4-49a5-f653-236a48ffe522",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 6.6774 - accuracy: 0.0705\n",
      "Epoch 2/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 5.8152 - accuracy: 0.1068\n",
      "Epoch 3/100\n",
      "377/377 [==============================] - 13s 35ms/step - loss: 4.9973 - accuracy: 0.1569\n",
      "Epoch 4/100\n",
      "377/377 [==============================] - 13s 35ms/step - loss: 4.1011 - accuracy: 0.2226\n",
      "Epoch 5/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 3.2412 - accuracy: 0.3200\n",
      "Epoch 6/100\n",
      "377/377 [==============================] - 16s 42ms/step - loss: 2.5379 - accuracy: 0.4309\n",
      "Epoch 7/100\n",
      "377/377 [==============================] - 16s 43ms/step - loss: 2.0100 - accuracy: 0.5319\n",
      "Epoch 8/100\n",
      "377/377 [==============================] - 16s 41ms/step - loss: 1.6489 - accuracy: 0.6053\n",
      "Epoch 9/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 1.3757 - accuracy: 0.6688\n",
      "Epoch 10/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 1.1666 - accuracy: 0.7201\n",
      "Epoch 11/100\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 1.0503 - accuracy: 0.7444\n",
      "Epoch 12/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 1.0075 - accuracy: 0.7529\n",
      "Epoch 13/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 1.0802 - accuracy: 0.7294\n",
      "Epoch 14/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 1.1609 - accuracy: 0.7080\n",
      "Epoch 15/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 1.2657 - accuracy: 0.6772\n",
      "Epoch 16/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 1.0837 - accuracy: 0.7192\n",
      "Epoch 17/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 0.9879 - accuracy: 0.7456\n",
      "Epoch 18/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 0.8990 - accuracy: 0.7696\n",
      "Epoch 19/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.8549 - accuracy: 0.7817\n",
      "Epoch 20/100\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 0.8203 - accuracy: 0.7916\n",
      "Epoch 21/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.8130 - accuracy: 0.7877\n",
      "Epoch 22/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.8937 - accuracy: 0.7699\n",
      "Epoch 23/100\n",
      "377/377 [==============================] - 17s 46ms/step - loss: 1.0585 - accuracy: 0.72030s - loss: 1.0585 - accuracy: 0.72\n",
      "Epoch 24/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 1.1746 - accuracy: 0.6923\n",
      "Epoch 25/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 1.1311 - accuracy: 0.7039\n",
      "Epoch 26/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 0.9905 - accuracy: 0.7417\n",
      "Epoch 27/100\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 0.8852 - accuracy: 0.7692\n",
      "Epoch 28/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.8237 - accuracy: 0.7834\n",
      "Epoch 29/100\n",
      "377/377 [==============================] - 15s 41ms/step - loss: 0.7944 - accuracy: 0.7938\n",
      "Epoch 30/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.8413 - accuracy: 0.7809\n",
      "Epoch 31/100\n",
      "377/377 [==============================] - 16s 41ms/step - loss: 0.8833 - accuracy: 0.7672\n",
      "Epoch 32/100\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 0.9370 - accuracy: 0.7537\n",
      "Epoch 33/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 1.0715 - accuracy: 0.7180\n",
      "Epoch 34/100\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 1.0581 - accuracy: 0.7216\n",
      "Epoch 35/100\n",
      "377/377 [==============================] - 16s 43ms/step - loss: 1.0097 - accuracy: 0.7300\n",
      "Epoch 36/100\n",
      "377/377 [==============================] - 15s 41ms/step - loss: 0.9405 - accuracy: 0.7542\n",
      "Epoch 37/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.8833 - accuracy: 0.7659\n",
      "Epoch 38/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.8218 - accuracy: 0.7841\n",
      "Epoch 39/100\n",
      "377/377 [==============================] - 16s 43ms/step - loss: 0.7786 - accuracy: 0.7994\n",
      "Epoch 40/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.8003 - accuracy: 0.7901\n",
      "Epoch 41/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.8594 - accuracy: 0.7767\n",
      "Epoch 42/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.9713 - accuracy: 0.7501\n",
      "Epoch 43/100\n",
      "377/377 [==============================] - 15s 41ms/step - loss: 1.0435 - accuracy: 0.7284\n",
      "Epoch 44/100\n",
      "377/377 [==============================] - 15s 41ms/step - loss: 1.0286 - accuracy: 0.7331\n",
      "Epoch 45/100\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 0.9592 - accuracy: 0.74611s - los\n",
      "Epoch 46/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 0.8710 - accuracy: 0.7719\n",
      "Epoch 47/100\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 0.8502 - accuracy: 0.7819\n",
      "Epoch 48/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.8015 - accuracy: 0.7932\n",
      "Epoch 49/100\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 0.7916 - accuracy: 0.7930\n",
      "Epoch 50/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.9099 - accuracy: 0.7674\n",
      "Epoch 51/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 1.0097 - accuracy: 0.7357\n",
      "Epoch 52/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 1.0396 - accuracy: 0.7318\n",
      "Epoch 53/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.9969 - accuracy: 0.7412\n",
      "Epoch 54/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 0.9159 - accuracy: 0.7585\n",
      "Epoch 55/100\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 0.8436 - accuracy: 0.77702s - loss: 0.8\n",
      "Epoch 56/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 0.8047 - accuracy: 0.7873\n",
      "Epoch 57/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.7869 - accuracy: 0.7939\n",
      "Epoch 58/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.7909 - accuracy: 0.7922\n",
      "Epoch 59/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.8458 - accuracy: 0.7754\n",
      "Epoch 60/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.9214 - accuracy: 0.7625\n",
      "Epoch 61/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 1.0124 - accuracy: 0.7449\n",
      "Epoch 62/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 1.0290 - accuracy: 0.7367\n",
      "Epoch 63/100\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 0.9596 - accuracy: 0.7489\n",
      "Epoch 64/100\n",
      "377/377 [==============================] - 15s 41ms/step - loss: 0.8979 - accuracy: 0.7626\n",
      "Epoch 65/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.8318 - accuracy: 0.7796\n",
      "Epoch 66/100\n",
      "377/377 [==============================] - 16s 42ms/step - loss: 0.7980 - accuracy: 0.7857 ETA\n",
      "Epoch 67/100\n",
      "377/377 [==============================] - 16s 43ms/step - loss: 0.8122 - accuracy: 0.7829\n",
      "Epoch 68/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.8103 - accuracy: 0.7884\n",
      "Epoch 69/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.8163 - accuracy: 0.7872\n",
      "Epoch 70/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.8424 - accuracy: 0.77541s -\n",
      "Epoch 71/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.9130 - accuracy: 0.7623\n",
      "Epoch 72/100\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.9316 - accuracy: 0.75740s - loss: 0.9284 - accuracy\n",
      "Epoch 73/100\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 0.9227 - accuracy: 0.7574\n",
      "Epoch 74/100\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.9372 - accuracy: 0.7520\n",
      "Epoch 75/100\n",
      "377/377 [==============================] - 16s 41ms/step - loss: 0.8850 - accuracy: 0.7647\n",
      "Epoch 76/100\n",
      "377/377 [==============================] - 17s 44ms/step - loss: 0.8472 - accuracy: 0.77321s - los\n",
      "Epoch 77/100\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 0.9865 - accuracy: 0.7533\n",
      "Epoch 78/100\n",
      "377/377 [==============================] - 16s 42ms/step - loss: 1.0764 - accuracy: 0.7358\n",
      "Epoch 79/100\n",
      "377/377 [==============================] - 16s 42ms/step - loss: 0.9412 - accuracy: 0.7599\n",
      "Epoch 80/100\n",
      "377/377 [==============================] - 16s 43ms/step - loss: 0.8987 - accuracy: 0.7716\n",
      "Epoch 81/100\n",
      "377/377 [==============================] - 16s 41ms/step - loss: 0.8389 - accuracy: 0.7792\n",
      "Epoch 82/100\n",
      "377/377 [==============================] - 16s 42ms/step - loss: 0.8220 - accuracy: 0.7865\n",
      "Epoch 83/100\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 0.7926 - accuracy: 0.7951\n",
      "Epoch 84/100\n",
      "377/377 [==============================] - 19s 52ms/step - loss: 0.8323 - accuracy: 0.7908\n",
      "Epoch 85/100\n",
      "377/377 [==============================] - 22s 58ms/step - loss: 0.8701 - accuracy: 0.7745\n",
      "Epoch 86/100\n",
      "377/377 [==============================] - 22s 58ms/step - loss: 0.9552 - accuracy: 0.7574\n",
      "Epoch 87/100\n",
      "377/377 [==============================] - 20s 52ms/step - loss: 0.9987 - accuracy: 0.7495\n",
      "Epoch 88/100\n",
      "377/377 [==============================] - 18s 49ms/step - loss: 0.9137 - accuracy: 0.7649\n",
      "Epoch 89/100\n",
      "377/377 [==============================] - 20s 53ms/step - loss: 0.8495 - accuracy: 0.78041s - loss: - ETA: 0s - loss: 0.8425 - accu\n",
      "Epoch 90/100\n",
      "377/377 [==============================] - 19s 52ms/step - loss: 0.8232 - accuracy: 0.7890\n",
      "Epoch 91/100\n",
      "377/377 [==============================] - 19s 49ms/step - loss: 0.7903 - accuracy: 0.79460s - loss: 0\n",
      "Epoch 92/100\n",
      "377/377 [==============================] - 18s 48ms/step - loss: 0.8207 - accuracy: 0.7877\n",
      "Epoch 93/100\n",
      "377/377 [==============================] - 18s 48ms/step - loss: 0.8801 - accuracy: 0.7769\n",
      "Epoch 94/100\n",
      "377/377 [==============================] - 18s 48ms/step - loss: 0.9184 - accuracy: 0.7635\n",
      "Epoch 95/100\n",
      "377/377 [==============================] - 18s 49ms/step - loss: 0.9138 - accuracy: 0.7632TA: 4s - ETA: 4s - loss: 0.8 - E\n",
      "Epoch 96/100\n",
      "377/377 [==============================] - 18s 46ms/step - loss: 0.9108 - accuracy: 0.7647\n",
      "Epoch 97/100\n",
      "377/377 [==============================] - 19s 50ms/step - loss: 0.8592 - accuracy: 0.7751\n",
      "Epoch 98/100\n",
      "223/377 [================>.............] - ETA: 7s - loss: 0.7736 - accuracy: 0.7950"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "history = model.fit(x_train, y_train, epochs=100, verbose=1)\n",
    "#print model.summary()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YXGelKThoTT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "poeprYK8h-c7",
    "outputId": "d6509d2c-55b9-4da5-afc7-47f4a5293bd9"
   },
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Vc6PHgxa6Hm",
    "outputId": "676f43b9-c565-48f3-8637-5315782edf4d"
   },
   "outputs": [],
   "source": [
    "seed_text = \"I've got a bad feeling about this\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Course 3 - Week 4 - Lesson 2 - Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
