{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ph5eir3Pf-3z"
   },
   "source": [
    "# Constructing a Text Generation Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GbGfr_oLCat"
   },
   "source": [
    "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aHK2CYygXom"
   },
   "source": [
    "## Import TensorFlow and related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2LmLTREBf5ng"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmLTO_dpgge9"
   },
   "source": [
    "## Get the Dataset\n",
    "\n",
    "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Bf5FVHfganK",
    "outputId": "7b2ca2e5-bac6-426e-9f90-34b43c865f07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-02 10:35:02--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
      "Resolving drive.google.com (drive.google.com)... 172.217.204.101, 172.217.204.100, 172.217.204.113, ...\n",
      "Connecting to drive.google.com (drive.google.com)|172.217.204.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8oc6cin6ubqdt7nfemc2lgsbtqt0fjcd/1606905300000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-12-02 10:35:05--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8oc6cin6ubqdt7nfemc2lgsbtqt0fjcd/1606905300000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
      "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 173.194.216.132, 2607:f8b0:400c:c12::84\n",
      "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|173.194.216.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘/tmp/songdata.csv’\n",
      "\n",
      "/tmp/songdata.csv       [   <=>              ]  69.08M   121MB/s    in 0.6s    \n",
      "\n",
      "2020-12-02 10:35:06 (121 MB/s) - ‘/tmp/songdata.csv’ saved [72436445]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
    "    -O /tmp/songdata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saf1hkRfBo76"
   },
   "source": [
    "Text generation can be done through simply predicting the next most likely word, given an input sequence. This can be done over and over by feeding the original input sequence, plus the newly predicted end word, as the next input sequence to the model. As such, the full output generated from a very short original input can effectively go on however long you want it to be.\n",
    "\n",
    "The only real change to the network here is the output layer will now be equivalent to a node per each possible new word to generate - so, if you have 1,000 possible words in your corpus, you’d have an output array of length 1,000. You’ll also need to change the loss function from binary cross-entropy to categorical cross entropy - before, we had only a 0 or 1 as output, now there are potentially thousands of output “classes” (each possible word).\n",
    "The example from the video with “Welcome to the…” as the input, with different probabilities per word as the output. Text Generation takes an input and outputs probabilities for the next most probable word.\n",
    "\n",
    "https://video.udacity-data.com/topher/2020/March/5e6fc0b3_prediction-example/prediction-example.png\n",
    "\n",
    "Text Generation takes an input and outputs probabilities for the next most probable word.\n",
    "\n",
    "\n",
    "https://botnik.org/content/harry-potter.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gu1BTzMIS1oy"
   },
   "source": [
    "## **First 10 Songs**\n",
    "\n",
    "Let's first look at just 10 songs from the dataset, and see how things perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmb9rGaAUDO-"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apcEXp7WhVBs",
    "outputId": "987eca43-d84a-4f06-f7bd-e006ad0b87c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
      "495\n"
     ]
    }
   ],
   "source": [
    "# Step 1 : Core of the process tokenize the words\n",
    "# Read the dataset from csv - just first 10 songs for now\n",
    "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
    "\n",
    "# Create the corpus using the 'text' column containing lyrics\n",
    "corpus = create_lyrics_corpus(dataset, 'text')\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenizer = tokenize_corpus(corpus)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 19\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2AVAvyF_Vuh5"
   },
   "outputs": [],
   "source": [
    "def create_lyrics_corpus(dataset, field):\n",
    "  # Remove all other punctuation\n",
    "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
    "  # Make it lowercase\n",
    "  dataset[field] = dataset[field].str.lower()\n",
    "  # Make it one long string to split by line\n",
    "  lyrics = dataset[field].str.cat()\n",
    "  corpus = lyrics.split('\\n')\n",
    "  # Remove any trailing whitespace\n",
    "  for l in range(len(corpus)):\n",
    "    corpus[l] = corpus[l].rstrip()\n",
    "  # Remove any empty lines\n",
    "  corpus = [l for l in corpus if l != '']\n",
    "\n",
    "  return corpus\n",
    "\n",
    "def tokenize_corpus(corpus, num_words=-1):\n",
    "  # Fit a Tokenizer on the corpus\n",
    "  if num_words > -1:\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "  else:\n",
    "    tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DR6AoGi0CXCZ"
   },
   "source": [
    "As noted before, there are hardly any differences in the model itself, other than changing the number of nodes in the output layer and changing the loss function.\n",
    "\n",
    "The more obvious changes come in working with the input and output data. The input data takes chunks of sequences and just splits off the final word as its label. So, if we had the sentence “I went to the beach with my dog”, and we had a max input length of five words, we’d get:\n",
    "\n",
    "    Input: I went to the beach\n",
    "\n",
    "    Label: with\n",
    "\n",
    "Now, that’s not the only sequence that will come from the sentence! We would also get:\n",
    "\n",
    "    Input: went to the beach with\n",
    "\n",
    "    Label: my\n",
    "\n",
    "And:\n",
    "\n",
    "    Input: to the beach with my\n",
    "\n",
    "    Label: dog\n",
    "\n",
    "That’s how the N-Grams used in the pre-processing work - a single input sequence might actually become a series of sequences and labels.\n",
    "\n",
    "With the output of the network, I’ll let you mostly investigate that code in the Colab on the next page, but the important thing there is that you can keep looping and creating more text by just appending the next word onto the previous input sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9x68iN_X6FK"
   },
   "source": [
    "### Create Sequences and Labels\n",
    "\n",
    "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QmlTsUqfikVO"
   },
   "outputs": [],
   "source": [
    "# Step 2: The word sequence and label on which to train\n",
    "sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tsequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences for equal input length \n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
    "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
    "# One-hot encode the labels\n",
    "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zsmu3aEId49i",
    "outputId": "87f5d91d-bab6-4c53-ad7f-73c2fed5c5b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "97\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
      "   4]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
      " 287]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Check out how some of our data is being stored\n",
    "# The Tokenizer has just a single index per word\n",
    "print(tokenizer.word_index['know'])\n",
    "print(tokenizer.word_index['feeling'])\n",
    "# Input sequences will have multiple indexes\n",
    "print(input_sequences[5])\n",
    "print(input_sequences[6])\n",
    "# And the one hot labels will be as long as the full spread of tokenized words\n",
    "print(one_hot_labels[5])\n",
    "print(one_hot_labels[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1TAJMlmfO8r"
   },
   "source": [
    "### Train a Text Generation Model\n",
    "\n",
    "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
    "\n",
    "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1YXuxIqfygN"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXVFpoREhV6Y"
   },
   "source": [
    "### View the Training Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "aeSNfS7uhch0",
    "outputId": "1b507817-69dc-401d-fa71-549bac6626c3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rAgRpxYhjpB"
   },
   "source": [
    "### Generate new lyrics!\n",
    "\n",
    "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DC7zfcgviDTp",
    "outputId": "22b66389-e2c7-4e1c-8759-f91963d75364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im feeling chills and i had to plan out every way she sees me girl feel fine past start park life cool eye touch touch touch touch touch more do every advice crazy girl do do what could without i do every do every do found eyes do eyes never do found could fight life am be life had before realized life had making life had making had making making making before me sound sound life sees me sound sound sound life better park life had making making making me sound sound sound sound life better better touch cool sucker life but but\n"
     ]
    }
   ],
   "source": [
    "# Step 3 : Predict new words\n",
    "seed_text = \"im feeling chills\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
